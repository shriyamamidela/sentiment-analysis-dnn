
# Sentiment Analysis using Deep Neural Networks

This project explores and compares the performance of four different deep learning architecturesâ€”**CNN**, **LSTM**, **MLP**, and **Autoencoder**â€”for binary sentiment classification of tweets (positive or negative).


## ğŸ“Œ Objective
To classify tweet sentiments (positive or negative) using deep learning models and compare their performance across different architectures and hyperparameter settings.

## ğŸ“Š Dataset
- **Source:** [Twitter Sentiment Analysis Dataset â€“ Kaggle](https://www.kaggle.com/datasets/abhi8923shriv/sentiment-analysis-dataset?select=train.csv)
- **Filename:** `train.csv`
- **Format:** CSV
- **Encoding:** ISO-8859-1
- **Classes:** `positive`, `negative` (neutral excluded)
- **Used Features:** `text`, `sentiment`
- **Dropped/Unused Columns:** `time of tweet`, `age of user`, `country`, `population`, `land area`, `density`


## ğŸ› ï¸ Technologies Used
- Python, PyTorch, torchtext
- pandas, scikit-learn, NumPy

## ğŸ§¹ Preprocessing Steps
- Removed URLs, special characters, digits
- Lowercased text
- Tokenized using `torchtext`
- Built vocabulary with `<unk>` for rare words
- Encoded sentiment: `negative â†’ 0`, `positive â†’ 1`
- Padded tweets to a fixed token length

## ğŸ§  Models Implemented
### 1. CNN (Convolutional Neural Network)
- Used Conv1D layers with varying kernel sizes
- Tuned hyperparameters: `embed_dim`, `num_filters`, `kernel_size`, `learning_rate`
- Best Accuracy: **92.19%** (train), **84.97%** (test)

### 2. LSTM (Long Short-Term Memory)
- Bidirectional LSTM with 1â€“2 layers
- Hyperparameter tuning done for `embed_dim`, `hidden_dim`, `num_layers`, `learning_rate`
- Best Accuracy: **86.25%**

### 3. MLP (Multi-Layer Perceptron)
- Used averaged embeddings with 2 dense layers
- Best Accuracy: **85.36%**

### 4. Autoencoder-Based Classifier
- Encoder compresses to bottleneck â†’ classifier head
- Decoder exists but unused in classification
- Best Accuracy: **87.28%**

## ğŸ“ˆ Result Summary
| Model        | Best Test Accuracy |
|--------------|--------------------|
| CNN          | 84.97%             |
| LSTM         | 86.25%             |
| MLP          | 85.36%             |
| Autoencoder  | 87.28%             |

## ğŸ“š Report
Detailed report is available in `DNN_report.docx`.

---

<pre> ## ğŸ“¦ File Structure ``` sentiment-analysis-dnn/ â”œâ”€â”€ Sentiment_Models.ipynb # Jupyter Notebook with all model code â”œâ”€â”€ DNN_report.docx # Detailed project report â”œâ”€â”€ train.csv # Twitter sentiment dataset â””â”€â”€ README.md # Project overview and documentation ``` </pre>


---

## ğŸƒâ€â™€ï¸ How to Run
1. Clone the repo  
   `git clone https://github.com/shriyamamidela/sentiment-analysis-dnn.git`

2. Install dependencies  
   `pip install torch pandas scikit-learn`

3. Run the notebook  
   Open `Sentiment_Models.ipynb` and run all cells.

---

## ğŸ’¡ Future Work
- Integrate pre-trained embeddings (like GloVe)
- Try attention-based models (e.g., Transformers)
- Deploy as a web application

---

